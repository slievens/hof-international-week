{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Computes the output of a single head of attention, inclusive of the output projection.\n",
    "    The shape of the input tensors is expected to be (batch_size, time_steps, model_dim).\n",
    "    The shape of the output tensor will be (batch_size, time_steps, model_dim).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_k:int, d_v:int=None, causal:bool=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Constructor for the SingleHeadAttention layer.\n",
    "\n",
    "        d_k: int, the dimension of the key and query vectors.\n",
    "        d_v: int, the dimension of the value vectors. If None, it is set to d_k.\n",
    "        causal: bool, whether to apply a causal mask to the attention scores.\n",
    "\n",
    "        Note: the learnable weights are added to the layer in the `build` method, once we know dimension of the embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        # Store the parameters for the layer.\n",
    "        self.d_k = d_k\n",
    "        ## YOUR CODE HERE\n",
    "        # Precompute the scaling factor (1/sqrt(d_k)) for the attention scores.\n",
    "        self.scaling_factor = ## YOUR CODE HERE\n",
    "\n",
    "    def build(self, batch_input_shape_list):\n",
    "        \"\"\"\n",
    "        Add the learnable weights to the layer.\n",
    "\n",
    "        batch_input_shape_list: list of input shapes. Can be used to infer the shape of the weights.\n",
    "        \"\"\"\n",
    "        Q_shape,  *_ = batch_input_shape_list\n",
    "        batch_size, time_steps, model_dim = Q_shape\n",
    "\n",
    "        # Add weights in the order WQ, WK, WV, WO\n",
    "        self.WQ = self.add_weight(shape=(model_dim, self.d_k), initializer='he_normal')\n",
    "        ## YOUR CODE HERE\n",
    "        \n",
    "\n",
    "    def call(self, inputs:list):\n",
    "        \"\"\"\n",
    "        Inputs should be a list of 2 or 3 tensors, in the order Q, K, V.\n",
    "        If only two tensors are provided, the second tensor is used for both K and V.\n",
    "        \"\"\"\n",
    "        if len(inputs) == 3:\n",
    "            Q, K, V = inputs\n",
    "        elif len(inputs) == 2:\n",
    "            Q, K = inputs\n",
    "            V = K\n",
    "\n",
    "        # Compute the scores using matrix multiplications.\n",
    "        score = ## YOUR CODE HERE\n",
    "\n",
    "        if self.causal:\n",
    "            # Apply the causal mask to the scores. \n",
    "            # Steps: \n",
    "            # 1. fill a tensor with -1e9 (a very large negative number) \n",
    "            # 2. Take the upper triangular part of the tensor\n",
    "            # 3. Add this to the original scores.\n",
    "            \n",
    "            ## YOUR CODE HERE\n",
    "\n",
    "        # Compute the attention weights using softmax.\n",
    "        attn_weigths = ## YOUR CODE HERE\n",
    "\n",
    "        # Take linear combinations of the values and apply the output projection using matrix multiplication.\n",
    "        ## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMultiHeadAttention(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Simple implementation of multi-head attention.\n",
    "    Is uses $h$ instances of SingleHeadAttention and sums their outputs in a loop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_k, d_v=None, num_heads=1, causal=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Store the parameters for the layer.\n",
    "        ## YOUR CODE HERE\n",
    "    \n",
    "        # Create the list of SingleHeadAttention instances.\n",
    "        ## YOUR CODE HERE\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Inputs should be a list of 2 or 3 tensors, in the order Q, K, V.\n",
    "        If only two tensors are provided, the second tensor is used for both K and V.\n",
    "        \"\"\"\n",
    "\n",
    "        ## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for `SingleHeadAttention`\n",
    "\n",
    "Execute the cell below to test the `SingleHeadAttention` class. \n",
    "This is done by comparing the results with that of Keras' built-in `MultiHeadAttention` class.\n",
    "The weights are copied from the Keras class to ensure that the results are comparable.\n",
    "\n",
    "You should see fairly small differences (hopefully zero) between the two outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_dim = 16\n",
    "num_heads = 1\n",
    "causal = False # Try both False and True\n",
    "my_attention = SingleHeadAttention(d_k=key_dim, causal=causal)\n",
    "keras_attention = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, use_bias=False)\n",
    "\n",
    "# Generate random data, batch of 100, 10 time steps, 32 features\n",
    "X = keras.random.normal(shape=(100,10,32))\n",
    "keras_output = keras_attention(X, X, X, use_causal_mask=causal)\n",
    "print(f\"Shape of keras output: {keras.ops.shape(keras_output)}\")\n",
    "print(\"Shape of keras weights\")\n",
    "for w in keras_attention.get_weights():\n",
    "    print(keras.ops.shape(w))\n",
    "\n",
    "my_output = my_attention([X, X, X])\n",
    "print(f\"Shape of SingleHeadAttention output: {keras.ops.shape(keras_output)}\")\n",
    "print(\"Shape of SingleHeadAttention weights\")\n",
    "for w in my_attention.get_weights():\n",
    "    print(keras.ops.shape(w))\n",
    "\n",
    "squeezed_weights = []\n",
    "for w in keras_attention.get_weights():\n",
    "    squeezed_weights.append(keras.ops.squeeze(w))\n",
    "my_attention.set_weights(squeezed_weights)\n",
    "my_output = my_attention([X, X, X])\n",
    "print(f\"Largest absolute difference between outputs: {keras.ops.max(keras.ops.abs(my_output - keras_output))}\")\n",
    "print(f\"Largest relative difference between outputs: {keras.ops.max(keras.ops.abs(my_output - keras_output) / keras.ops.abs(keras_output))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for `SimpleMultiHeadAttention`\n",
    "\n",
    "Execute the cell below to test the `SimpleMultiHeadAttention` class. \n",
    "This is done by comparing the results with that of Keras' built-in `MultiHeadAttention` class.\n",
    "The weights are copied from the Keras class to ensure that the results are comparable.\n",
    "\n",
    "You should see fairly small differences between the two outputs, although the relative differences may be a bit larger.\n",
    "This is probably due to the fact that the results are summed in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_dim = 16\n",
    "num_heads = 2\n",
    "causal = True # Try both False and True\n",
    "my_attention = SimpleMultiHeadAttention(num_heads=num_heads, d_k=key_dim, causal=causal)\n",
    "keras_attention = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, use_bias=False)\n",
    "X = keras.random.normal(shape=(200,10,32))\n",
    "keras_output = keras_attention(X, X, X, use_causal_mask=causal)\n",
    "print(f\"Shape of keras output: {keras.ops.shape(keras_output)}\")\n",
    "print(\"Shape of keras weights\")\n",
    "for w in keras_attention.get_weights():\n",
    "    print(keras.ops.shape(w))\n",
    "\n",
    "my_output = my_attention([X, X, X])\n",
    "print(f\"Shape of SimpleMultiHeadAttention output: {keras.ops.shape(keras_output)}\")\n",
    "print(\"Shape of SimpleMultiHeadAttention weights\")\n",
    "for w in my_attention.get_weights():\n",
    "    print(keras.ops.shape(w))\n",
    "\n",
    "# Put the weights in the right shape and order for SimpleMultiHeadAttention\n",
    "weights_for_heads = []\n",
    "for idx in range(num_heads):\n",
    "    for w in keras_attention.get_weights()[:-1]:\n",
    "        weights_for_heads.append(w[:, idx, :])\n",
    "    weights_for_heads.append(keras_attention.get_weights()[-1][idx, :,:])\n",
    "\n",
    "my_attention.set_weights(weights_for_heads)\n",
    "my_output = my_attention([X, X, X])\n",
    "print(f\"Largest absolute difference between outputs: {keras.ops.max(keras.ops.abs(my_output - keras_output))}\")\n",
    "print(f\"Largest relative difference between outputs: {keras.ops.max(keras.ops.abs(my_output - keras_output) / keras.ops.abs(keras_output))}\")\n",
    "#assert keras.ops.all(keras.ops.isclose(keras_output, my_output)), \"Outputs are not close enough\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiHead Attention with Einstein Summation\n",
    "\n",
    "Implement a second version of `SimpleMultiHeadAttention` using Einstein summation.\n",
    "You should not rely on `SingleHeadAttention` for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EinopsMultiHeadAttention(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Implementation of multi-head attention using einops.einsum.\n",
    "\n",
    "    No (explicit) loops are used in this implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_k, d_v=None, num_heads=1, causal=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Constructor for the SingleHeadAttention layer.\n",
    "\n",
    "        d_k: int, the dimension of the key and query vectors.\n",
    "        d_v: int, the dimension of the value vectors. If None, it is set to d_k.\n",
    "        num_heads: int, the number of attention heads.\n",
    "        causal: bool, whether to apply a causal mask to the attention scores.\n",
    "\n",
    "        Note: the learnable weights are added to the layer in the `build` method, once we know dimension of the embeddings.\n",
    "        \"\"\"\n",
    "        # Call the parent constructor \n",
    "        super().__init__(**kwargs)\n",
    "        # Store the parameters for the layer.\n",
    "        ## YOUR CODE HERE\n",
    "        # Precompute the scaling factor (1/sqrt(d_k)) for the attention scores.\n",
    "        self.scaling_factor = ## YOUR CODE HERE\n",
    "    \n",
    "    def build(self, batch_input_shape_list):\n",
    "        \"\"\"\n",
    "        Add the learnable weights to the layer. The shapes of the weights are inferred from the input shapes, \n",
    "        and are identical to the shape of the weights in the Keras implementation.\n",
    "\n",
    "        batch_input_shape_list: list of input shapes. Can be used to infer the shape of the weights.\n",
    "        \"\"\"\n",
    "        Q_shape, *_ = batch_input_shape_list\n",
    "        _, _, model_dim = Q_shape\n",
    "\n",
    "        # Add weights in the order WQ, WK, WV, WO\n",
    "        # The weighs have the shape (model_dim, num_heads, d_k) for Q, K and (model_dim, num_heads, d_v) for V\n",
    "        # The output projection has shape (num_heads, d_v, model_dim)\n",
    "        ## YOUR CODE HERE\n",
    "\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Inputs should be a list of 2 or 3 tensors, in the order Q, K, V.\n",
    "        If only two tensors are provided, the second tensor is used for both K and V.\n",
    "        \"\"\"\n",
    "        if len(inputs) == 3:\n",
    "            Q, K, V = inputs\n",
    "        elif len(inputs) == 2:\n",
    "            Q, K = inputs\n",
    "            V = K\n",
    "\n",
    "        # Compute the queries, keys and values for each head using einsum.\n",
    "        # qs should have shape (b, h, t_q, d_k)\n",
    "        # ks should have shape (b, h, t_k, d_k)\n",
    "        # vs should have shape (b, h, t_k, d_v)            \n",
    "        qs = ## YOUR CODE HERE\n",
    "        ks = ## YOUR CODE HERE\n",
    "        vs = ## YOUR CODE HERE\n",
    "\n",
    "        # Compute the (unnormalized) scores using einsum.\n",
    "        # scores should have shape (b, h, t_q, t_k)\n",
    "        score = ## YOUR CODE HERE\n",
    "        if self.causal:\n",
    "             # Apply the causal mask to the scores. \n",
    "            # Steps: \n",
    "            # 1. fill a tensor with -1e9 (a very large negative number) \n",
    "            # 2. Take the upper triangular part of the tensor\n",
    "            # 3. Add this to the original scores. Rely on broadcasting\n",
    "            ## YOUR CODE HERE\n",
    "        \n",
    "        # Compute the attention weights by applying softmax along the last axis.\n",
    "        attn_weigths = ## YOUR CODE HERE\n",
    "\n",
    "        # Compute the linear combinations of the \"rows\" in vs using einsum. Result should have shape (b, h, t_q, d_v).\n",
    "        mixed_vs = ## YOUR CODE HERE\n",
    "        # Compute the output projection using einsum. Result should have shape (b, t_q, d_model).\n",
    "        result = ## YOUR CODE HERE\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for `EinopsMultiHeadAttention`\n",
    "\n",
    "Execute the cell below to test the `EinopsMultiHeadAttention` class. \n",
    "This is done by comparing the results with that of Keras' built-in `MultiHeadAttention` class.\n",
    "The weights are copied from the Keras class to ensure that the results are comparable.\n",
    "\n",
    "You should see very small differences (ideally zero) between the two outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_dim = 64\n",
    "num_heads = 4\n",
    "causal = True # Try both False and True\n",
    "my_attention = EinopsMultiHeadAttention(num_heads=num_heads, d_k=key_dim, causal=causal)\n",
    "keras_attention = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, use_bias=False)\n",
    "X = keras.random.normal(shape=(200,10,32))\n",
    "keras_output = keras_attention(X, X, X, use_causal_mask=causal)\n",
    "print(f\"Shape of keras output: {keras.ops.shape(keras_output)}\")\n",
    "print(\"Shape of keras weights\")\n",
    "for w in keras_attention.get_weights():\n",
    "    print(keras.ops.shape(w))\n",
    "\n",
    "my_output = my_attention([X, X, X])\n",
    "print(f\"Shape of EinopsMultiHeadAttention output: {keras.ops.shape(keras_output)}\")\n",
    "print(\"Shape of EinopsMultiHeadAttention weights\")\n",
    "for w in my_attention.get_weights():\n",
    "    print(keras.ops.shape(w))\n",
    "\n",
    "# Copy the weights from keras to the einops version\n",
    "my_attention.set_weights(keras_attention.get_weights())\n",
    "my_output = my_attention([X, X, X])\n",
    "print(f\"Largest absolute difference between outputs: {keras.ops.max(keras.ops.abs(my_output - keras_output))}\")\n",
    "print(f\"Largest relative difference between outputs: {keras.ops.max(keras.ops.abs(my_output - keras_output) / keras.ops.abs(keras_output))}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
